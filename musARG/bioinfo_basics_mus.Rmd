---
title: "bioinfo_basics_mus"
author: "Nicole Adams"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
# Load libraries
library(kableExtra)  # make nice tables
library(tidyverse)  # data wrangling

set.seed(123)  # Set seed for reproducibility


# Define Rmd knit options
knitr::opts_knit$set(root.dir = "/Users/rachel/Documents/NicoleAdams/bioinfo_help/musARG")
knitr::opts_chunk$set(
	fig.height = 6,
	fig.width = 8,
	warning = FALSE, # supress warnings
	message = FALSE # supress messages
)

# Create a template that sets eval=FALSE for bash chunks
knitr::opts_template$set(nobash = list(eval = FALSE))

# Set this template as default for all chunks initially
knitr::opts_chunk$set(opts.label = "nobash")

```

Tutorial-esq document for familiarizing folks with CLI, genomic data, and the UMich Great Lakes cluster specifically for the Mus musculus ARG project with Ben Carlson and Alex Lewanski

# Great Lakes cluster
## Sign on via terminal (Mac) or VScode (PC)
```{bash}
ssh <uniquename>@greatlakes.arc-ts.umich.edu
```

Will need to enter UM password and do 2FA. If not connected to MWireless or UM internet, need to connect to [UM VPN](https://its.umich.edu/enterprise/wifi-networks/vpn/getting-started) first before ssh'ing.

# Mus dataset
The Michael Nachman Mus dataset downloaded via Globus though a shared collections link from Beth Dumont at The Jackson Laboratory.

Navigate to Turbo (storage drive for active data connected to Great Lakes)
```{bash}
 cd /nfs/turbo/lsa-bradburd/
```

Make a new directory for the Mus ARG project and move into it
```{bash}
mkdir houseMouse_ARG

cd houseMouse_ARG

# check which directory you are in
pwd
```

Temporarily Mus files are in scratch/ on Great Lakes

## Check for any truncated .bam files
Use samtools quickcheck to look for files that didn't download properly
```{bash}
cd /scratch/bradburd_root/bradburd0/shared_data/mus_datafiles

module load Bioinformatics samtools

find . -type f -name "*.bam" -print0 | xargs -0 samtools quickcheck -v 2> bad_bams.fofn

```

### Get list of complete bam files
```{bash}
# List of all bam files
find . -type f -name "*.bam" > bams.txt
less bams.txt | wc -l # 188

# List of *merged_file.bam
find . -type f -name "*merged_file.bam" > merged_file_bams.txt
less merged_file_bams.txt |wc -l # 118

# List of *dedup.bam
find . -type f -name "*dedup.bam" > dedup_bams.txt
less dedup_bams.txt |wc -l # 70

```

# Get stats on bam files
Run samtools stats to get mapping quality information
```{bash}
# Test one example -- takes a long time
samtools stats A_J/ERR9880121_dedup.bam > ERR9880121_dedup_samstats.txt

```

## Build a SLURM script to submit the samtools stats as a job -- hard coded file
(/nfs/turbo/lsa-bradburd/houseMouse_ARG/code/samstats.sh)
```{bash}
#!/bin/bash
#SBATCH --job-name samstats
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=3
##SBATCH --mem=1G
#SBATCH --time=01:00:00
##SBATCH --account=bradburd0
#SBATCH --partition=standard
#SBATCH --output=samstats.%j.out
#SBATCH --error=samstats.%j.err
#SBATCH --mail-type=BEGIN,FAIL,END
#SBATCH --mail-user=nicolead@umich.edu

# Usage: sbatch /nfs/turbo/lsa-bradburd/houseMouse_ARG/code/samstats.sh


module load Bioinformatics samtools

# make sure in correct directory - w bam files
cd /scratch/bradburd_root/bradburd0/shared_data/mus_datafiles


# hard code file by writing it in script
samtools stats --threads 3 A_J/ERR9880121_dedup.bam > ERR9880121_dedup_samstats.txt

# -OR-

# test one bam file from the list of dedup bam files -- use a Shell Command Substitution
samtools stats --threads 3 $(head -1 dedup_bams.txt) > ERR9880121_dedup_samstats.txt

```

## Build a SLURM script to submit the samtools stats as a job -- w input parameters!
(/nfs/turbo/lsa-bradburd/houseMouse_ARG/scripts/samstats_input.sh)
Different ways:
1. Place loop inside the slurm script - execute sequentially
2. Make loop part of submitting the job so each file exectutes in parallel
3. Make an array so that ^

Additionally 
1. Add logfile dir to Turbo dir - before script
2. Add output dir to Turbo dir (and samstats) - in script
3. Input variables
```{bash}
#!/bin/bash
#SBATCH --job-name samstats
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=3
##SBATCH --mem=1G
#SBATCH --time=01:00:00
##SBATCH --account=bradburd0
#SBATCH --partition=standard
#SBATCH --output=/nfs/turbo/lsa-bradburd/houseMouse_ARG/logfiles/samstats.%j.out
#SBATCH --error=/nfs/turbo/lsa-bradburd/houseMouse_ARG/logfiles/samstats.%j.err
#SBATCH --mail-type=BEGIN,FAIL,END
#SBATCH --mail-user=nicolead@umich.edu

module load Bioinformatics samtools

OUTDIR="/nfs/turbo/lsa-bradburd/houseMouse_ARG/output/samtools_stats"

mkdir -p $OUTDIR

# Loop inside SLURM script
# Usage: sbatch code/samstats_input.sh <list_of_bams.txt>
# Example: sbatch code/samstats_input.sh dedup_bams.txt

BAMLIST=$1

cat $BAMLIST | while read BAM; do
    FILENAME=$(basename "$BAM") # Extract the filename (e.g., DRR271602_dedup.bam)
    OUTNAME="${FILENAME%.bam}" # Remove the .bam extension (e.g., DRR271602_dedup)
    samtools stats --threads $SLURM_NTASKS_PER_NODE $BAM > $OUTDIR/${OUTNAME}_samstats.txt;
    done


#### -OR- #### 


# Loop outside SLURM script - submit each samtools stats as a separate job
# Usage: cat dedup_bams.txt | while read BAM; do
#    FILENAME=$(basename "$BAM") # Extract the filename (e.g., DRR271602_dedup.bam)
#    OUTNAME="${FILENAME%.bam}" # Remove the .bam extension (e.g., DRR271602_dedup)
#    sbatch code/samstats_input.sh $BAM $OUTNAME
#    done

BAM=$1
OUTNAME=$2

samtools stats --threads $SLURM_NTASKS_PER_NODE $BAM > $OUTDIR/${OUTNAME}_samstats.txt


#### -OR- ####


# Array job - submit samtools stats as a separate array job
# Usage: sbatch code/samstats_array.sh -OR- sbatch samstats_array.sh --array 1-70

# Chaage Output/Error logging (Use the array ID %A_%a to keep logs separate) - # %A is the Job ID, %a is the Array Task ID
#SBATCH --ntasks-per-node=1 # change for array
#SBATCH --cpus-per-task=3 # change for array
#SBATCH --output=/nfs/turbo/lsa-bradburd/houseMouse_ARG/logfiles/samstats_%A_%a.out
#SBATCH --error=/nfs/turbo/lsa-bradburd/houseMouse_ARG/logfiles/samstats_%A_%a.err
#SBATCH --array=1-70 #command-line flags provided to sbatch command always take precedence over this line

BAMLIST=$1

# Correlate the bam file with the array task ID
BAM_PATH=$(tail -n +$SLURM_ARRAY_TASK_ID $BAMLIST | head -n 1)

# Extract the base name for the output file
FILENAME=$(basename "$BAM_PATH")
OUTNAME="${FILENAME%.bam}" # e.g., DRR271602_dedup

# Run samtools command for this single task
samtools stats --threads $SLURM_CPUS_PER_TASK $BAM_PATH > $OUTDIR/${OUTNAME}_samstats.txt


```

Combine samtools stats into one file
```{bash}
# combine with multiqc
module load Bioinformatics multiqc

cd /nfs/turbo/lsa-bradburd/houseMouse_ARG/output/samtools_stats

multiqc *samstats.txt --filename samtools_stats_multiq

```



# Mus musculus reference genome
Make a new directory to house reference files, and move into it
```{bash}
cd /nfs/turbo/lsa-bradburd/houseMouse_ARG/

mkdir reference

cd reference
```

Get FTP link from NCBI genome > FTP > "index of ___". You want the *.fna.gz reference file, but there are additional files that may be useful in analyses down the road, which you can download later as needed.

(There are multiple programs to use to download, I'll use wget bc you only need to do this once, but you can use whatever you like: rsync, ncbi_datasets cli, etc)
```{bash}
# Mouse reference genome FTP path (just for ease in coding)
ftp_base="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.27_GRCm39"

# Download key files
wget "${ftp_base}/GCF_000001635.27_GRCm39_genomic.fna.gz"           # Genome FASTA
#wget "${ftp_base}/GCF_000001635.27_GRCm39_genomic.gff.gz"           # Annotations
#wget "${ftp_base}/GCF_000001635.27_GRCm39_cds_from_genomic.fna.gz"  # CDS sequences

# Download assembly report
wget "${ftp_base}/GCF_000001635.27_GRCm39_assembly_report.txt"

```

Index reference genome with bwa index and create .fai file with samtools
bwa index builds the Burrows-Wheeler Transform (BWT) index, which allows BWA to efficiently search and map reads to the reference sequence. Generates several files (e.g., .amb, .ann, .bwt, .pac, .sa)
samtools faidx creates a .fai file that also allows for fast, random access to subsequences within a larger FASTA (or FASTQ) file.

Since indexing can take a while, put in sbatch file and submit as a job
```{bash}
#!/bin/bash
#SBATCH --job-name refIndex
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=4G
#SBATCH --time=01:00:00
##SBATCH --account=bradburd0
#SBATCH --partition=standard
#SBATCH --output=refIndex.%j.out
#SBATCH --error=refIndex.%j.err
#SBATCH --mail-type=BEGIN,FAIL,END
#SBATCH --mail-user=nicolead@umich.edu

module load Bioinformatics bwa samtools htslib

# bwa index needs an unzipped file
gunzip GCF_003704035.1_HU_Pman_2.1.3_genomic.fna.gz

bwa index GCF_003704035.1_HU_Pman_2.1.3_genomic.fna #bwa index not multithreaded

samtools faidx GCF_003704035.1_HU_Pman_2.1.3_genomic.fna  #samtools faidx not multithreaded

# re-compress the reference genome
bgzip GCF_003704035.1_HU_Pman_2.1.3_genomic.fna
```

